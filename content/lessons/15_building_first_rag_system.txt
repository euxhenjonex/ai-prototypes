=== BUILDING YOUR FIRST RAG SYSTEM: A PRACTICAL GUIDE ===

Ready to build your first Retrieval Augmented Generation (RAG) system? This guide walks you through every step with code examples, common mistakes, and debugging tips. By the end, you'll have a working RAG application!

## What You'll Build

A RAG system that:
1. Loads documents (your knowledge base)
2. Splits them into manageable chunks
3. Creates vector embeddings
4. Stores embeddings in a vector database
5. Retrieves relevant chunks based on user questions
6. Generates answers using an LLM with retrieved context

**Real-world example:** A company documentation chatbot that answers employee questions about policies, procedures, and benefits.

## Prerequisites

**Required knowledge:**
- Basic Python (functions, classes, imports)
- Understanding of APIs
- Familiarity with virtual environments

**Tools you'll need:**
- Python 3.8+
- OpenAI API key (or alternative LLM API)
- Text editor or IDE (VS Code recommended)

## Step 1: Project Setup

### Create Project Structure

```bash
mkdir my-rag-system
cd my-rag-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Create directories
mkdir data
mkdir src

# Create files
touch src/main.py
touch .env
touch requirements.txt
```

### Install Dependencies

**requirements.txt:**
```
langchain==0.1.0
langchain-openai==0.0.2
langchain-community==0.0.10
faiss-cpu==1.7.4
python-dotenv==1.0.0
tiktoken==0.5.2
```

```bash
pip install -r requirements.txt
```

### Configure Environment

**.env:**
```
OPENAI_API_KEY=sk-your-api-key-here
```

**Crucial:** Add `.env` to `.gitignore` to avoid exposing secrets!

## Step 2: Prepare Your Data

Create sample documents in `data/` directory:

**data/company_policies.txt:**
```
Company Vacation Policy

All full-time employees are entitled to 20 days of paid vacation per year.
Part-time employees receive vacation days proportional to hours worked.
Vacation days must be requested at least 2 weeks in advance.
Unused vacation days can be carried over to the next year, up to a maximum of 5 days.

Remote Work Policy

Employees may work remotely up to 3 days per week with manager approval.
A home office stipend of $500 per year is provided for remote workers.
Employees must be available during core hours (10 AM - 3 PM) regardless of location.
```

**data/benefits.txt:**
```
Health Insurance

The company provides comprehensive health insurance coverage.
Coverage includes medical, dental, and vision.
Monthly premium: $200 for individual, $400 for family.
No deductible for preventive care.

Retirement Plan

401(k) plan with company matching up to 5% of salary.
Vesting period: 3 years for company contributions.
Employees can contribute up to $22,500 per year (2024 limit).
```

**Tip:** Start with 2-3 small documents to test. Scale up later.

## Step 3: Basic RAG Implementation

### src/main.py

```python
import os
from pathlib import Path
from dotenv import load_dotenv

# LangChain imports
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in .env file!")

# Configuration
DATA_DIR = Path("data")
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
MODEL_NAME = "gpt-4o-mini"
TEMPERATURE = 0.3  # Lower = more factual

def load_documents(data_dir: Path):
    """Load all text documents from directory"""
    print(f"üìÇ Loading documents from {data_dir}...")

    loader = DirectoryLoader(
        str(data_dir),
        glob="**/*.txt",
        loader_cls=TextLoader
    )

    documents = loader.load()
    print(f"‚úì Loaded {len(documents)} documents")

    return documents

def split_documents(documents):
    """Split documents into chunks"""
    print(f"‚úÇÔ∏è Splitting documents into chunks...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)
    print(f"‚úì Created {len(chunks)} chunks")

    return chunks

def create_vectorstore(chunks):
    """Create FAISS vector store from chunks"""
    print(f"üßÆ Creating embeddings and vector store...")

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_documents(chunks, embeddings)

    print(f"‚úì Vector store created")
    return vectorstore

def create_qa_chain(vectorstore):
    """Create Question-Answering chain"""
    print(f"üîó Creating QA chain...")

    # Custom prompt template
    template = """Use the following context to answer the question.
    If you don't know the answer, say "I don't have enough information to answer that."
    Don't make up information.

    Context:
    {context}

    Question: {question}

    Answer:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    # Initialize LLM
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=TEMPERATURE,
        openai_api_key=OPENAI_API_KEY
    )

    # Create retriever
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}  # Retrieve top 3 most relevant chunks
    )

    # Create QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # "stuff" = put all context in one prompt
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    print(f"‚úì QA chain ready")
    return qa_chain

def main():
    """Main RAG pipeline"""
    print("üöÄ Starting RAG System...\n")

    # Step 1: Load documents
    documents = load_documents(DATA_DIR)

    # Step 2: Split into chunks
    chunks = split_documents(documents)

    # Step 3: Create vector store
    vectorstore = create_vectorstore(chunks)

    # Step 4: Create QA chain
    qa_chain = create_qa_chain(vectorstore)

    print("\n‚úÖ RAG System Ready!\n")

    # Interactive query loop
    print("Ask questions (type 'quit' to exit):\n")

    while True:
        question = input("Question: ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("Goodbye! üëã")
            break

        if not question:
            continue

        print("\nü§î Thinking...\n")

        try:
            result = qa_chain({"query": question})
            answer = result["result"]
            sources = result["source_documents"]

            print(f"Answer: {answer}\n")

            # Optionally show sources
            print(f"üìö Based on {len(sources)} document chunks")

        except Exception as e:
            print(f"‚ùå Error: {str(e)}\n")

if __name__ == "__main__":
    main()
```

### Run Your RAG System

```bash
python src/main.py
```

**Expected output:**
```
üöÄ Starting RAG System...

üìÇ Loading documents from data...
‚úì Loaded 2 documents
‚úÇÔ∏è  Splitting documents into chunks...
‚úì Created 8 chunks
üßÆ Creating embeddings and vector store...
‚úì Vector store created
üîó Creating QA chain...
‚úì QA chain ready

‚úÖ RAG System Ready!

Ask questions (type 'quit' to exit):

Question: How many vacation days do employees get?

ü§î Thinking...

Answer: All full-time employees are entitled to 20 days of paid vacation per year.
Part-time employees receive vacation days proportional to hours worked.

üìö Based on 3 document chunks
```

**Success!** You've built a working RAG system! üéâ

## Step 4: Understanding the Code

### Document Loading

```python
loader = DirectoryLoader(str(data_dir), glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
```

**What it does:**
- Scans directory for `.txt` files
- Loads content into Document objects
- Each document has: `page_content` (text) and `metadata` (source file, etc.)

**Common issue:** File encoding errors
**Solution:**
```python
loader = TextLoader(file_path, encoding='utf-8')
```

### Document Splitting

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

**Why split?**
- LLMs have token limits (can't process entire book at once)
- Smaller chunks = more precise retrieval
- Overlap prevents losing context at chunk boundaries

**Chunk size considerations:**
- **Too small (100-200)**: May lose context
- **Too large (2000+)**: Less precise retrieval
- **Sweet spot (500-1000)**: Good balance

**Overlap (50-100 tokens):** Ensures sentences aren't cut off mid-thought

### Embeddings and Vector Store

```python
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)
```

**What happens:**
1. Each chunk is sent to OpenAI's embedding API
2. Returns a 1536-dimensional vector (for `text-embedding-ada-002`)
3. FAISS indexes these vectors for fast similarity search

**Cost:** ~$0.0001 per 1K tokens (very cheap for initial setup)

### Retrieval

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

**How it works:**
1. User asks: "How many vacation days?"
2. Question is embedded (converted to vector)
3. FAISS finds 3 most similar chunk vectors
4. Returns corresponding text chunks

**Parameters:**
- `k=3`: Number of chunks to retrieve (adjust based on chunk size)
- `search_type="similarity"`: Use cosine similarity (default)
- Alternative: `search_type="mmr"` (Maximum Marginal Relevance - more diverse results)

### QA Chain

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True
)
```

**Chain types:**
- **"stuff"**: Put all context in one prompt (best for small contexts)
- **"map_reduce"**: Process chunks separately, then combine (for large contexts)
- **"refine"**: Iteratively refine answer with each chunk

**"stuff" is usually best:** Simpler, faster, more coherent answers

## Step 5: Common Mistakes and Debugging

### Mistake 1: Empty or Missing Documents

**Symptom:** "Loaded 0 documents"

**Causes:**
- Wrong file path
- No `.txt` files in directory
- Files in subdirectories but glob pattern is wrong

**Fix:**
```python
# Debug: Print what files were found
import glob
files = glob.glob(str(DATA_DIR / "**/*.txt"), recursive=True)
print(f"Found files: {files}")
```

### Mistake 2: API Key Issues

**Symptom:** `AuthenticationError: Invalid API key`

**Fixes:**
- Check `.env` file exists and has correct key
- Ensure `load_dotenv()` is called before using key
- Verify key starts with `sk-` and has no extra spaces
- Check environment: `print(os.getenv("OPENAI_API_KEY"))`

### Mistake 3: Poor Retrieval Quality

**Symptom:** Answers are irrelevant or "I don't know" when info exists

**Debugging:**
```python
# Print retrieved chunks
result = qa_chain({"query": question})
sources = result["source_documents"]

for i, doc in enumerate(sources):
    print(f"\n--- Chunk {i+1} ---")
    print(doc.page_content[:200])
```

**Solutions:**
- Adjust chunk size (try 300-1000)
- Increase `k` (retrieve more chunks)
- Improve document quality (clearer writing, better structure)
- Try `search_type="mmr"` for diversity

### Mistake 4: Slow Performance

**Symptom:** Takes 10+ seconds per query

**Causes:**
- Too many chunks being retrieved
- Large chunks sent to LLM
- Slow LLM model

**Optimizations:**
- Use `gpt-4o-mini` instead of `gpt-4` (10x faster, cheaper)
- Reduce `k` from 5 to 3
- Use smaller chunk sizes
- Cache embeddings (don't regenerate on every run)

### Mistake 5: Hallucinations

**Symptom:** LLM makes up information not in documents

**Solutions:**
- Lower temperature (0.0-0.3 for factual queries)
- Improve prompt: "Only use the context provided. If unsure, say so."
- Show source documents to user for verification
- Use `return_source_documents=True` and verify sources match answer

## Step 6: Improvements and Extensions

### Persist Vector Store (Avoid Re-embedding)

```python
import os

VECTORSTORE_PATH = "vectorstore"

def get_or_create_vectorstore(chunks):
    """Load existing vectorstore or create new one"""
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    if os.path.exists(VECTORSTORE_PATH):
        print("üì¶ Loading existing vector store...")
        vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings)
    else:
        print("üßÆ Creating new vector store...")
        vectorstore = FAISS.from_documents(chunks, embeddings)
        vectorstore.save_local(VECTORSTORE_PATH)
        print("üíæ Vector store saved")

    return vectorstore
```

**Benefits:**
- Instant startup (no re-embedding)
- Save API costs
- Faster iteration during development

### Add Metadata for Better Retrieval

```python
def load_documents_with_metadata(data_dir: Path):
    """Load documents with metadata"""
    documents = []

    for file_path in data_dir.glob("**/*.txt"):
        loader = TextLoader(str(file_path))
        docs = loader.load()

        # Add metadata
        for doc in docs:
            doc.metadata.update({
                "source": file_path.name,
                "type": "policy" if "policy" in file_path.name.lower() else "benefit"
            })

        documents.extend(docs)

    return documents

# Later, filter retrieval by metadata
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 3,
        "filter": {"type": "policy"}  # Only retrieve policy documents
    }
)
```

### Conversation Memory

Enable multi-turn conversations:

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# Now follow-up questions work:
# User: "How many vacation days do I get?"
# Bot: "20 days for full-time employees."
# User: "Can I carry them over?" ‚Üê Bot remembers context
# Bot: "Yes, up to 5 days can be carried over."
```

### Web UI with Streamlit

```python
# streamlit_app.py
import streamlit as st
from main import create_rag_system

st.title("ü§ñ Company Policies Chatbot")

# Initialize RAG system (cache it)
@st.cache_resource
def load_rag():
    return create_rag_system()

qa_chain = load_rag()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

if prompt := st.chat_input("Ask a question about company policies..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Get bot response
    result = qa_chain({"query": prompt})
    answer = result["result"]

    # Add bot message
    st.session_state.messages.append({"role": "assistant", "content": answer})
    with st.chat_message("assistant"):
        st.write(answer)
```

Run: `streamlit run streamlit_app.py`

### Support Multiple File Types

```python
from langchain_community.document_loaders import (
    TextLoader,
    PDFLoader,
    Docx2txtLoader,
    CSVLoader
)

def load_document(file_path: Path):
    """Load document based on file type"""
    suffix = file_path.suffix.lower()

    loaders = {
        ".txt": TextLoader,
        ".pdf": PDFLoader,
        ".docx": Docx2txtLoader,
        ".csv": CSVLoader
    }

    loader_class = loaders.get(suffix)
    if not loader_class:
        raise ValueError(f"Unsupported file type: {suffix}")

    loader = loader_class(str(file_path))
    return loader.load()
```

## Best Practices Summary

1. **Start simple:** Get basic RAG working first, then optimize
2. **Use small test datasets:** Faster iteration, easier debugging
3. **Monitor costs:** Embeddings and LLM calls add up - cache when possible
4. **Version your data:** Track which documents are in your knowledge base
5. **Validate retrieval:** Always check if correct chunks are being retrieved
6. **Test with real questions:** Use actual user queries, not synthetic ones
7. **Iterate on prompts:** Prompt engineering significantly impacts quality
8. **Handle errors gracefully:** Network failures, API errors, rate limits
9. **Log everything:** Log queries, retrieval results, errors for analysis
10. **Measure quality:** Track answer accuracy, user satisfaction

## Troubleshooting Checklist

**No results / "I don't know" answers:**
- [ ] Check if documents loaded correctly
- [ ] Verify chunks are reasonable size
- [ ] Print retrieved chunks to see what's being found
- [ ] Try increasing `k` (more chunks)
- [ ] Check if question wording matches document language

**Slow performance:**
- [ ] Use `gpt-4o-mini` instead of `gpt-4`
- [ ] Cache vector store (don't recreate each time)
- [ ] Reduce number of chunks retrieved
- [ ] Consider using local embeddings model

**High costs:**
- [ ] Cache embeddings (don't re-embed same documents)
- [ ] Use fewer chunks in retrieval
- [ ] Switch to cheaper LLM for non-critical queries
- [ ] Set `max_tokens` limits

**Hallucinations:**
- [ ] Lower temperature (0.0-0.3)
- [ ] Improve prompt (emphasize "only use context")
- [ ] Show source documents for verification
- [ ] Use fact-checking step

## Next Steps

You've built a functional RAG system! Here's how to take it further:

1. **Deploy it:** Build API with FastAPI, deploy to cloud
2. **Add evaluation:** Measure accuracy with test questions
3. **Try other vector DBs:** Pinecone, Weaviate for production scale
4. **Advanced retrieval:** Hybrid search, reranking, query expansion
5. **Multi-modal RAG:** Support images, tables, charts
6. **Agents:** Let LLM decide when to use RAG vs other tools

**Resources:**
- LangChain documentation: https://python.langchain.com/
- Vector database comparisons
- RAG evaluation frameworks (RAGAS, TruLens)

Congratulations on building your first RAG system! üéâ Keep experimenting and building! üöÄ
