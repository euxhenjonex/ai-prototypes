# ğŸš€ LangChain Mini-RAG API

A minimal **Retrieval-Augmented Generation (RAG)** API built with **FastAPI**, **LangChain**, and **OpenAI**.  
This project demonstrates how to connect a local knowledge base (FAISS vector store) with an OpenAI model to answer questions contextually.

---

## ğŸ§  Overview

This prototype is part of a collection of **AI Engineering demos**, showing practical use cases of language models and vector databases.

The goal is to build a **lightweight RAG pipeline** â€” from document ingestion to semantic search and contextual response generation â€” fully operational through a simple REST API.

---

## âš™ï¸ Tech Stack

- ğŸ§© **FastAPI** â€” modern Python framework for building APIs  
- ğŸ§  **LangChain** â€” handles the RAG logic and document pipelines  
- ğŸ” **FAISS** â€” local vector database for semantic similarity search  
- ğŸ¤– **OpenAI API** â€” LLM reasoning and text generation  
- ğŸ”‘ **python-dotenv** â€” for secure API key management  

---

## ğŸ”— API Endpoints

| Method | Endpoint | Description |
|:--------|:----------|:-------------|
| `GET` | `/` | Health check |
| `POST` | `/query` | Query your local knowledge base |

---

## ğŸ§ª Example Request

```json
{
  "question": "What is Artificial Intelligence?"
}
```

### Example Response
```json
{
  "question": "What is Artificial Intelligence?",
  "answer": "Artificial Intelligence is the field focused on creating machines capable of human-like reasoning and learning."
}
```

ğŸš€ Run Locally
1ï¸âƒ£ Clone the repository
git clone https://github.com/euxhenjonex/ai-prototypes.git
cd ai-prototypes/02-langchain-rag

2ï¸âƒ£ Create and activate a virtual environment
python3 -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ **Set your OpenAI API key**

Create a `.env` file in the project root:
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

5ï¸âƒ£ **Run the API**
```bash
uvicorn main:app --reload
```

Then open:  
ğŸ‘‰ **http://127.0.0.1:8000/docs** (or whatever port is available)

---

## ğŸ§± How It Works

1. **Document loading** â€” reads text files from `sample_data/`
2. **Chunking & Embedding** â€” splits text and converts it into vector embeddings
3. **FAISS Indexing** â€” stores embeddings for semantic retrieval
4. **LLM Querying** â€” combines user question + retrieved context to generate an informed answer

---