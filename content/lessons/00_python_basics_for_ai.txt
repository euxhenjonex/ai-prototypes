=== PYTHON BASICS FOR AI ENGINEERING ===

Welcome to your journey into AI Engineering! Before diving into advanced topics like machine learning and neural networks, it's essential to understand the Python fundamentals that will serve as your foundation.

## Why Python for AI?

Python has become the de facto language for AI and machine learning for several compelling reasons:

1. **Simplicity and Readability**: Python's syntax is clean and intuitive, allowing you to focus on solving problems rather than wrestling with complex syntax.

2. **Rich Ecosystem**: Python boasts an extensive collection of libraries specifically designed for AI, including NumPy, pandas, TensorFlow, PyTorch, and scikit-learn.

3. **Community Support**: With millions of developers worldwide, you'll find abundant resources, tutorials, and solutions to common problems.

4. **Versatility**: Python works seamlessly across different platforms (Windows, macOS, Linux) and integrates well with other technologies.

## Essential Python Concepts for AI

### 1. Virtual Environments

Virtual environments are isolated Python spaces that allow you to manage dependencies for different projects independently.

**Why Use Virtual Environments?**
- Prevents conflicts between different project dependencies
- Keeps your global Python installation clean
- Makes projects reproducible and shareable
- Essential for production deployments

**Creating a Virtual Environment:**

```bash
# Using venv (built into Python 3.3+)
python -m venv myenv

# Activate on macOS/Linux
source myenv/bin/activate

# Activate on Windows
myenv\Scripts\activate

# Deactivate when done
deactivate
```

**Best Practice:** Always create a virtual environment for each new project!

### 2. Package Management with pip and requirements.txt

pip is Python's package installer, used to install external libraries and frameworks.

**Installing Packages:**
```bash
# Install a single package
pip install numpy

# Install a specific version
pip install tensorflow==2.13.0

# Install from requirements file
pip install -r requirements.txt
```

**Creating requirements.txt:**

A `requirements.txt` file lists all the packages your project needs, making it easy for others (or your future self) to recreate your environment.

```bash
# Generate requirements.txt from current environment
pip freeze > requirements.txt
```

**Example requirements.txt:**
```
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
tensorflow==2.13.0
```

**Pro Tip:** Pin your versions (use ==) for reproducibility, or use >= for flexibility with patch updates.

### 3. Data Structures for AI

Understanding Python's built-in data structures is crucial for handling AI data efficiently.

**Lists:** Ordered, mutable collections
```python
# Training data samples
data_points = [1.5, 2.3, 3.7, 4.2]
labels = ["cat", "dog", "cat", "bird"]

# Common operations
data_points.append(5.1)  # Add element
first_point = data_points[0]  # Access by index
```

**Dictionaries:** Key-value pairs, perfect for configuration
```python
# Model configuration
config = {
    "learning_rate": 0.001,
    "epochs": 100,
    "batch_size": 32,
    "optimizer": "adam"
}

# Access values
lr = config["learning_rate"]
```

**NumPy Arrays:** The backbone of numerical computing in AI
```python
import numpy as np

# Create arrays
data = np.array([[1, 2, 3], [4, 5, 6]])

# Mathematical operations (vectorized, very fast!)
mean = np.mean(data)
normalized = (data - mean) / np.std(data)
```

### 4. Functions and Modularity

In AI projects, you'll write many functions to process data, train models, and evaluate results.

**Basic Function:**
```python
def preprocess_data(raw_data):
    """
    Clean and normalize input data.

    Args:
        raw_data: List or array of raw values

    Returns:
        Normalized data
    """
    mean = np.mean(raw_data)
    std = np.std(raw_data)
    return (raw_data - mean) / std

# Usage
clean_data = preprocess_data(training_data)
```

**Lambda Functions** (useful for quick transformations):
```python
# Traditional function
def square(x):
    return x ** 2

# Lambda equivalent (anonymous function)
square = lambda x: x ** 2

# Common use: with map
squared_values = list(map(lambda x: x ** 2, [1, 2, 3, 4]))
# Result: [1, 4, 9, 16]
```

### 5. Asynchronous Programming (async/await)

Many modern AI applications involve API calls, which can be slow. Asynchronous programming allows your code to do other work while waiting for responses.

**Why Async Matters in AI:**
- Making multiple API calls to LLMs (like OpenAI's GPT)
- Processing data streams in real-time
- Building responsive web applications with FastAPI
- Handling concurrent user requests efficiently

**Basic Async Example:**
```python
import asyncio

async def fetch_ai_response(prompt):
    """
    Simulate calling an AI API (like OpenAI)
    """
    print(f"Sending prompt: {prompt}")
    await asyncio.sleep(2)  # Simulates API call delay
    return f"AI response to: {prompt}"

async def process_multiple_prompts():
    """
    Process multiple prompts concurrently
    """
    prompts = [
        "Explain neural networks",
        "What is backpropagation?",
        "Describe transformers"
    ]

    # Run all requests concurrently
    tasks = [fetch_ai_response(prompt) for prompt in prompts]
    responses = await asyncio.gather(*tasks)

    return responses

# Run the async function
# asyncio.run(process_multiple_prompts())
```

**Key Async Concepts:**
- `async def`: Defines an asynchronous function
- `await`: Pauses execution until the awaited operation completes
- `asyncio.gather()`: Runs multiple async operations concurrently
- **Benefits:** Instead of waiting 6 seconds (3 prompts × 2 seconds each), you wait only 2 seconds!

### 6. Working with APIs

APIs (Application Programming Interfaces) are essential in modern AI applications. You'll frequently interact with cloud AI services like OpenAI, Hugging Face, or your own deployed models.

**Making API Calls with requests:**
```python
import requests

# POST request to an AI API
def query_ai_model(question):
    url = "https://api.example.com/v1/query"
    headers = {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    }
    data = {
        "question": question,
        "max_tokens": 100
    }

    response = requests.post(url, headers=headers, json=data)

    # Check if successful
    if response.status_code == 200:
        return response.json()
    else:
        return {"error": f"API error: {response.status_code}"}

# Usage
result = query_ai_model("What is machine learning?")
print(result)
```

**Environment Variables for Security:**

Never hardcode API keys! Use environment variables instead.

```python
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

API_KEY = os.getenv("OPENAI_API_KEY")

if not API_KEY:
    raise ValueError("API key not found! Check your .env file")
```

**.env file example:**
```
OPENAI_API_KEY=sk-your-secret-key-here
MODEL_NAME=gpt-4o-mini
MAX_TOKENS=500
```

### 7. Error Handling

Robust error handling is crucial in AI applications where many things can go wrong (API failures, invalid data, model errors).

```python
def safe_api_call(prompt):
    try:
        response = call_ai_api(prompt)
        return response
    except requests.exceptions.Timeout:
        return {"error": "API request timed out"}
    except requests.exceptions.ConnectionError:
        return {"error": "Could not connect to API"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}
```

### 8. File Operations for AI Projects

AI projects involve reading datasets, saving models, and logging results.

**Reading Data Files:**
```python
# Read text data
with open("training_data.txt", "r", encoding="utf-8") as f:
    text_data = f.read()

# Read CSV with pandas
import pandas as pd
df = pd.read_csv("dataset.csv")

# Read JSON
import json
with open("config.json", "r") as f:
    config = json.load(f)
```

**Saving Model Results:**
```python
# Save predictions
predictions = model.predict(test_data)
np.save("predictions.npy", predictions)

# Save metrics
metrics = {
    "accuracy": 0.95,
    "loss": 0.23,
    "f1_score": 0.93
}
with open("metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)
```

## Practical Example: Building a Simple AI Data Pipeline

Let's combine these concepts into a realistic AI workflow:

```python
import os
import numpy as np
import pandas as pd
from dotenv import load_dotenv
import requests

# Load environment variables
load_dotenv()
API_KEY = os.getenv("AI_API_KEY")

def load_dataset(filepath):
    """Load and validate dataset"""
    try:
        df = pd.read_csv(filepath)
        print(f"✓ Loaded {len(df)} samples")
        return df
    except FileNotFoundError:
        print(f"✗ File not found: {filepath}")
        return None

def preprocess(data):
    """Clean and normalize data"""
    # Remove missing values
    clean_data = data.dropna()

    # Normalize numerical columns
    numerical_cols = data.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        mean = clean_data[col].mean()
        std = clean_data[col].std()
        clean_data[col] = (clean_data[col] - mean) / std

    return clean_data

async def get_ai_insights(data_summary):
    """Query AI API for insights"""
    url = "https://api.ai-service.com/analyze"
    headers = {"Authorization": f"Bearer {API_KEY}"}

    try:
        response = requests.post(
            url,
            headers=headers,
            json={"summary": data_summary},
            timeout=30
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}

def main():
    # 1. Load data
    data = load_dataset("data.csv")
    if data is None:
        return

    # 2. Preprocess
    clean_data = preprocess(data)

    # 3. Generate summary
    summary = {
        "total_samples": len(clean_data),
        "features": list(clean_data.columns),
        "mean_values": clean_data.mean().to_dict()
    }

    # 4. Get AI insights (would use async in production)
    # insights = asyncio.run(get_ai_insights(summary))

    # 5. Save results
    clean_data.to_csv("processed_data.csv", index=False)
    print("✓ Pipeline complete!")

if __name__ == "__main__":
    main()
```

## Best Practices Summary

1. **Always use virtual environments** - One per project
2. **Pin your dependencies** - Use requirements.txt
3. **Never hardcode secrets** - Use environment variables
4. **Write modular functions** - Each function does one thing well
5. **Handle errors gracefully** - Try-except blocks for external operations
6. **Document your code** - Use docstrings for functions
7. **Use type hints** - Helps catch bugs early (e.g., `def func(x: int) -> str:`)
8. **Keep it simple** - Write readable code over "clever" code

## Next Steps

Now that you understand Python basics for AI, you're ready to explore:
- **Machine Learning fundamentals** (supervised/unsupervised learning)
- **Deep Learning frameworks** (TensorFlow, PyTorch)
- **LangChain** for building LLM applications
- **FastAPI** for deploying AI models as APIs
- **Vector databases** for semantic search

Remember: The best way to learn is by building projects. Start small, experiment often, and don't be afraid to make mistakes - that's how you learn!

Happy coding! 🚀
